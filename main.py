from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.live import Live
from tools import tools
import os
import asyncio
from langgraph.checkpoint.memory import MemorySaver
import math
from openai import OpenAI
from model_config import get_model_context_length

load_dotenv()

console = Console()

MODEL = os.getenv("MODEL", "deepseek-chat")
VERSION = "0.2"
CWD = os.path.basename(os.getcwd())

# ä¸Šä¸‹æ–‡ç®¡ç†é…ç½®
MAX_CONTEXT_LIMIT = int(os.getenv("MAX_CONTEXT_LIMIT", "200000"))  # é»˜è®¤é™åˆ¶ 200k tokens
CONTEXT_WARNING_THRESHOLD = 0.8  # ä½¿ç”¨è¶…è¿‡ 80% æ—¶è­¦å‘Š
CONTEXT_CRITICAL_THRESHOLD = 0.95  # ä½¿ç”¨è¶…è¿‡ 95% æ—¶ä¸¥é‡è­¦å‘Š

llm = ChatOpenAI(model=MODEL, streaming=True)

memory = MemorySaver()
agent = create_agent(llm, tools, checkpointer=memory)
history: list[tuple[str, str]] = []

MODEL_CONTEXT_TOKENS: int | None = None
MODEL_CONTEXT_SOURCE = "uninitialized"

# ASCII Art Logo
LOGO = """    â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘
    â•šâ•â•     â•šâ•â•"""

def show_welcome():
    content = f"""[bold white]      Welcome![/bold white]

[bold cyan]{LOGO}[/bold cyan]

  [dim]{MODEL} Â· [cyan]~/{CWD}[/cyan][/dim]
  [dim]è¾“å…¥ [bold yellow]/help[/bold yellow] æŸ¥çœ‹å‘½ä»¤ Â· [bold yellow]exit[/bold yellow] é€€å‡º[/dim]"""

    panel = Panel(
        content,
        title=f"[bold orange1]Miles Coder[/bold orange1] [dim]v{VERSION}[/dim]",
        border_style="orange1",
        padding=(1, 2),
    )
    console.print(panel)


def find_context_length(payload: object, depth: int = 0) -> int | None:
    if payload is None or depth > 4:
        return None
    if isinstance(payload, dict):
        keys = (
            "context_length",
            "max_context_tokens",
            "max_input_tokens",
            "context_window",
            "context_window_size",
            "n_ctx",
        )
        for key in keys:
            value = payload.get(key)
            if isinstance(value, int) and value > 0:
                return value
            if isinstance(value, str) and value.isdigit():
                return int(value)
        for value in payload.values():
            if isinstance(value, (dict, list)):
                found = find_context_length(value, depth + 1)
                if found:
                    return found
        return None
    if isinstance(payload, list):
        for item in payload:
            found = find_context_length(item, depth + 1)
            if found:
                return found
        return None
    for attr in ("model_dump", "dict", "to_dict"):
        if hasattr(payload, attr):
            try:
                data = getattr(payload, attr)()
            except TypeError:
                continue
            return find_context_length(data, depth + 1)
    return None


def resolve_model_context_tokens() -> tuple[int | None, str]:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        # API key ä¸å­˜åœ¨æ—¶ï¼Œå°è¯•ä»é…ç½®è¡¨è·å–
        tokens = get_model_context_length(MODEL)
        return tokens, "config" if tokens else "unknown"

    base_url = os.getenv("OPENAI_BASE_URL")
    try:
        client = OpenAI(api_key=api_key, base_url=base_url)
    except Exception:
        # å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥æ—¶ï¼Œå°è¯•ä»é…ç½®è¡¨è·å–
        tokens = get_model_context_length(MODEL)
        return tokens, "config" if tokens else "unknown"

    model_info = None
    try:
        model_info = client.models.retrieve(MODEL)
    except Exception:
        model_info = None

    tokens = find_context_length(model_info)

    if not tokens:
        try:
            models = client.models.list()
            data = getattr(models, "data", None)
            if data:
                for item in data:
                    item_id = getattr(item, "id", None)
                    if item_id is None and isinstance(item, dict):
                        item_id = item.get("id")
                    if item_id == MODEL:
                        tokens = find_context_length(item)
                        break
        except Exception:
            tokens = None

    # å¦‚æœ API æ— æ³•è·å–ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä½¿ç”¨é…ç½®è¡¨ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
    if not tokens:
        tokens = get_model_context_length(MODEL)
        return tokens, "config" if tokens else "unknown"

    return tokens, "api" if tokens else "unknown"


def get_model_context_tokens() -> int | None:
    global MODEL_CONTEXT_TOKENS, MODEL_CONTEXT_SOURCE
    if MODEL_CONTEXT_SOURCE == "uninitialized":
        MODEL_CONTEXT_TOKENS, MODEL_CONTEXT_SOURCE = resolve_model_context_tokens()
    return MODEL_CONTEXT_TOKENS


def format_tokens(value: int) -> str:
    if value >= 100_000:
        return f"{value / 1000:.0f}k"
    if value >= 1_000:
        return f"{value / 1000:.1f}k"
    return str(value)


def estimate_tokens(text: str) -> int:
    if not text:
        return 0
    try:
        return llm.get_num_tokens(text)
    except Exception:
        ascii_chars = sum(1 for ch in text if ch.isascii())
        non_ascii_chars = len(text) - ascii_chars
        return max(1, math.ceil(ascii_chars / 4) + non_ascii_chars)


def estimate_tokens_from_messages(messages: list[tuple[str, str]]) -> int:
    if not messages:
        return 0
    combined = "\n".join(f"{role}: {content}" for role, content in messages)
    return estimate_tokens(combined)


def prompt_user_input() -> str:
    model_context_tokens = get_model_context_tokens()

    if model_context_tokens:
        # ä½¿ç”¨é…ç½®çš„ä¸Šä¸‹æ–‡é™åˆ¶æˆ–æ¨¡å‹è‡ªèº«çš„é™åˆ¶ï¼ˆå–è¾ƒå°å€¼ï¼‰
        effective_limit = min(model_context_tokens, MAX_CONTEXT_LIMIT)
        used_tokens = estimate_tokens_from_messages(history)
        usage_ratio = used_tokens / effective_limit
        percentage = int(usage_ratio * 100)

        # æ ¹æ®ä½¿ç”¨ç‡è®¾ç½®é¢œè‰²
        if usage_ratio >= CONTEXT_CRITICAL_THRESHOLD:
            color = "red bold"
            icon = "âš "
        elif usage_ratio >= CONTEXT_WARNING_THRESHOLD:
            color = "yellow"
            icon = "âš "
        else:
            color = "dim"
            icon = ""

        # æ˜¾ç¤ºç™¾åˆ†æ¯” + è¿›åº¦æ¡
        bar_width = 10
        filled = int(bar_width * usage_ratio)
        bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)

        ctx_label = f"[{color}]{icon} ctx {percentage}% [{bar}][/{color}]"
    else:
        ctx_label = "[dim]ctx unknown[/dim]"

    line = "â”€" * console.width
    console.print()
    console.print(line, style="dim orange1")  # ä¸Šçº¿
    console.print()  # è¾“å…¥è¡Œå ä½
    console.print(line, style="dim orange1")  # ä¸‹çº¿
    console.print(ctx_label)  # è¾“å…¥æ¡†å¤–å·¦ä¸‹è§’
    print("\033[3A", end="", flush=True)  # å…‰æ ‡ä¸Šç§»åˆ°è¾“å…¥è¡Œ
    user_input = console.input("[green]â€º [/green]")
    print("\033[2B", end="", flush=True)  # å…‰æ ‡ä¸‹ç§»åˆ°çŠ¶æ€è¡Œä¹‹å
    return user_input


def compact_history(keep_recent: int = 3) -> None:
    """
    å‹ç¼©å†å²è®°å½•ï¼Œåªä¿ç•™æœ€è¿‘çš„ N æ¡å¯¹è¯

    Args:
        keep_recent: ä¿ç•™æœ€è¿‘çš„å¯¹è¯è½®æ•°ï¼ˆé»˜è®¤ 3ï¼‰
    """
    global history

    if len(history) <= keep_recent:
        console.print(f"[dim]å†å²è®°å½•åªæœ‰ {len(history)} æ¡ï¼Œæ— éœ€å‹ç¼©[/dim]")
        return

    removed_count = len(history) - keep_recent
    old_history = history[:removed_count]

    # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯
    summary_parts = []
    for role, content in old_history:
        preview = content[:50].replace("\n", " ")
        if len(content) > 50:
            preview += "..."
        summary_parts.append(f"  - {role}: {preview}")

    summary = f"[å·²å‹ç¼© {removed_count} æ¡æ—©æœŸå¯¹è¯]\n" + "\n".join(summary_parts[:5])
    if len(summary_parts) > 5:
        summary += f"\n  ... è¿˜æœ‰ {len(summary_parts) - 5} æ¡"

    # ä¿ç•™æœ€è¿‘çš„å¯¹è¯ï¼Œå¹¶åœ¨å¼€å¤´æ·»åŠ æ±‡æ€»
    history = [("system", summary)] + history[-keep_recent:]

    before_tokens = estimate_tokens_from_messages(old_history + history[-keep_recent:])
    after_tokens = estimate_tokens_from_messages(history)
    saved_tokens = before_tokens - after_tokens

    console.print(
        f"[green]âœ“[/green] å·²å‹ç¼©å†å²è®°å½•ï¼šä¿ç•™æœ€è¿‘ {keep_recent} æ¡å¯¹è¯ï¼Œ"
        f"èŠ‚çœçº¦ {format_tokens(saved_tokens)} tokens"
    )


async def run_agent(user_input: str) -> str:
    content = ""
    last_response = ""
    config = {"configurable": {"thread_id": "main"}}

    with Live(console=console, refresh_per_second=10) as live:
        async for event in agent.astream_events(
            {"messages": [("user", user_input)]},
            config=config,
            version="v2"
        ):
            kind = event["event"]
            if kind == "on_chat_model_start":
                content = ""
            elif kind == "on_chat_model_stream":
                chunk = event["data"]["chunk"].content
                if chunk:
                    content += chunk
                    live.update(Markdown(content))
            elif kind == "on_chat_model_end":
                if content:
                    last_response = content
            elif kind == "on_tool_start":
                tool_name = event["name"]
                live.console.print(f"[dim]ğŸ”§ {tool_name}[/dim]")

    # è¾“å‡ºç»“æŸåæ·»åŠ ç©ºè¡Œ
    console.print()
    if content:
        last_response = content
    return last_response


if __name__ == "__main__":
    show_welcome()

    try:
        while True:
            user_input = prompt_user_input()

            if not user_input.strip():
                continue

            # å¤„ç†é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["exit", "quit"]:
                break

            # å¤„ç†å‹ç¼©å‘½ä»¤
            if user_input.lower() == "/compact":
                console.print()
                compact_history(keep_recent=3)
                continue

            # å¤„ç†æ¸…ç©ºå‘½ä»¤
            if user_input.lower() == "/clear":
                console.print()
                history.clear()
                console.print("[green]âœ“[/green] å·²æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•")
                continue

            # å¤„ç†å¸®åŠ©å‘½ä»¤
            if user_input.lower() == "/help":
                console.print()
                help_text = """[bold cyan]å¯ç”¨å‘½ä»¤ï¼š[/bold cyan]

  [yellow]/compact[/yellow]  - å‹ç¼©å†å²è®°å½•ï¼ˆä¿ç•™æœ€è¿‘ 3 æ¡å¯¹è¯ï¼‰
  [yellow]/clear[/yellow]    - æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•
  [yellow]/help[/yellow]     - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  [yellow]exit[/yellow] æˆ– [yellow]quit[/yellow] - é€€å‡ºç¨‹åº

[dim]ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡è¯´æ˜ï¼š[/dim]
  â€¢ [dim]0-79%[/dim]   - æ­£å¸¸ï¼ˆç°è‰²ï¼‰
  â€¢ [yellow]80-94%[/yellow]  - è­¦å‘Šï¼ˆé»„è‰² âš ï¼‰
  â€¢ [red bold]95-100%[/red bold] - ä¸¥é‡ï¼ˆçº¢è‰² âš ï¼‰ï¼Œå»ºè®®æ‰§è¡Œ /compact
"""
                console.print(help_text)
                continue

            console.print()
            assistant_response = asyncio.run(run_agent(user_input))
            history.append(("user", user_input))
            if assistant_response:
                history.append(("assistant", assistant_response))

    except KeyboardInterrupt:
        pass

    console.print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")
