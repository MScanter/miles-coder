from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage, SystemMessage
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.live import Live
from miles_coder.tools import tools
import os
import asyncio
import math
from openai import OpenAI
from miles_coder.model_config import get_model_context_length
from miles_coder.config import get_api_config, is_configured, setup_config

load_dotenv()

console = Console()

API_KEY, BASE_URL, MODEL = get_api_config()
VERSION = "0.2"
CWD = os.path.basename(os.getcwd())

# ä¸Šä¸‹æ–‡ç®¡ç†é…ç½®
MAX_CONTEXT_LIMIT = int(os.getenv("MAX_CONTEXT_LIMIT", "200000"))  # é»˜è®¤é™åˆ¶ 200k tokens
CONTEXT_WARNING_THRESHOLD = 0.8  # ä½¿ç”¨è¶…è¿‡ 80% æ—¶è­¦å‘Š
CONTEXT_CRITICAL_THRESHOLD = 0.95  # ä½¿ç”¨è¶…è¿‡ 95% æ—¶ä¸¥é‡è­¦å‘Š
SUMMARY_MESSAGE_CHAR_LIMIT = 800
SUMMARY_CHUNK_TOKEN_LIMIT = 2500

llm: ChatOpenAI | None = None
summary_llm: ChatOpenAI | None = None
agent = None


def init_llm():
    global llm, summary_llm, agent, API_KEY, BASE_URL, MODEL
    API_KEY, BASE_URL, MODEL = get_api_config()
    llm = ChatOpenAI(
        model=MODEL,
        streaming=True,
        api_key=API_KEY,
        base_url=BASE_URL,
    )
    summary_llm = ChatOpenAI(
        model=MODEL,
        streaming=False,
        temperature=0,
        api_key=API_KEY,
        base_url=BASE_URL,
    )
    agent = create_agent(llm, tools)


chat_messages: list[object] = []

MODEL_CONTEXT_TOKENS: int | None = None
MODEL_CONTEXT_SOURCE = "uninitialized"

# ASCII Art Logo
LOGO = """    â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘
    â•šâ•â•     â•šâ•â•"""

def show_welcome():
    content = f"""[bold white]      Welcome![/bold white]

[bold cyan]{LOGO}[/bold cyan]

  [dim]{MODEL} Â· [cyan]~/{CWD}[/cyan][/dim]
  [dim]è¾“å…¥ [bold yellow]/help[/bold yellow] æŸ¥çœ‹å‘½ä»¤ Â· [bold yellow]exit[/bold yellow] é€€å‡º[/dim]"""

    panel = Panel(
        content,
        title=f"[bold orange1]Miles Coder[/bold orange1] [dim]v{VERSION}[/dim]",
        border_style="orange1",
        padding=(1, 2),
    )
    console.print(panel)


def find_context_length(payload: object, depth: int = 0) -> int | None:
    if payload is None or depth > 4:
        return None
    if isinstance(payload, dict):
        keys = (
            "context_length",
            "max_context_tokens",
            "max_input_tokens",
            "context_window",
            "context_window_size",
            "n_ctx",
        )
        for key in keys:
            value = payload.get(key)
            if isinstance(value, int) and value > 0:
                return value
            if isinstance(value, str) and value.isdigit():
                return int(value)
        for value in payload.values():
            if isinstance(value, (dict, list)):
                found = find_context_length(value, depth + 1)
                if found:
                    return found
        return None
    if isinstance(payload, list):
        for item in payload:
            found = find_context_length(item, depth + 1)
            if found:
                return found
        return None
    for attr in ("model_dump", "dict", "to_dict"):
        if hasattr(payload, attr):
            try:
                data = getattr(payload, attr)()
            except TypeError:
                continue
            return find_context_length(data, depth + 1)
    return None


def resolve_model_context_tokens() -> tuple[int | None, str]:
    if not API_KEY:
        tokens = get_model_context_length(MODEL)
        return tokens, "config" if tokens else "unknown"

    try:
        client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    except Exception:
        # å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥æ—¶ï¼Œå°è¯•ä»é…ç½®è¡¨è·å–
        tokens = get_model_context_length(MODEL)
        return tokens, "config" if tokens else "unknown"

    model_info = None
    try:
        model_info = client.models.retrieve(MODEL)
    except Exception:
        model_info = None

    tokens = find_context_length(model_info)

    if not tokens:
        try:
            models = client.models.list()
            data = getattr(models, "data", None)
            if data:
                for item in data:
                    item_id = getattr(item, "id", None)
                    if item_id is None and isinstance(item, dict):
                        item_id = item.get("id")
                    if item_id == MODEL:
                        tokens = find_context_length(item)
                        break
        except Exception:
            tokens = None

    # å¦‚æœ API æ— æ³•è·å–ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä½¿ç”¨é…ç½®è¡¨ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
    if not tokens:
        tokens = get_model_context_length(MODEL)
        return tokens, "config" if tokens else "unknown"

    return tokens, "api" if tokens else "unknown"


def get_model_context_tokens() -> int | None:
    global MODEL_CONTEXT_TOKENS, MODEL_CONTEXT_SOURCE
    if MODEL_CONTEXT_SOURCE == "uninitialized":
        MODEL_CONTEXT_TOKENS, MODEL_CONTEXT_SOURCE = resolve_model_context_tokens()
    return MODEL_CONTEXT_TOKENS


def format_tokens(value: int) -> str:
    if value >= 100_000:
        return f"{value / 1000:.0f}k"
    if value >= 1_000:
        return f"{value / 1000:.1f}k"
    return str(value)


def estimate_tokens(text: str) -> int:
    if not text:
        return 0
    try:
        return llm.get_num_tokens(text)
    except Exception:
        ascii_chars = sum(1 for ch in text if ch.isascii())
        non_ascii_chars = len(text) - ascii_chars
        return max(1, math.ceil(ascii_chars / 4) + non_ascii_chars)


def _stringify_message_content(content: object) -> str:
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts = []
        for item in content:
            if isinstance(item, dict) and "text" in item:
                parts.append(str(item["text"]))
            else:
                parts.append(str(item))
        return " ".join(parts)
    if isinstance(content, dict):
        text = content.get("text")
        if isinstance(text, str):
            return text
    return str(content)


def _normalize_message(message: object) -> tuple[str, str]:
    if isinstance(message, tuple) and len(message) == 2:
        role, content = message
        return str(role), _stringify_message_content(content)
    if isinstance(message, dict):
        role = message.get("role") or message.get("type") or "unknown"
        content = message.get("content", "")
        return str(role), _stringify_message_content(content)
    role = getattr(message, "type", None)
    if role is not None and hasattr(message, "content"):
        return str(role), _stringify_message_content(getattr(message, "content"))
    return "unknown", _stringify_message_content(message)


def _is_user_role(role: str) -> bool:
    return role in ("user", "human")


def _is_assistant_role(role: str) -> bool:
    return role in ("assistant", "ai")


def _display_role(role: str) -> str:
    if role == "human":
        return "user"
    if role == "ai":
        return "assistant"
    return role


def _is_summary_system_message(role: str, content: str) -> bool:
    return role == "system" and content.startswith("[å·²å‹ç¼©")


def _get_message_name(message: object) -> str | None:
    if isinstance(message, dict):
        name = message.get("name")
        return name if isinstance(name, str) and name else None
    name = getattr(message, "name", None)
    return name if isinstance(name, str) and name else None


def _format_message_for_summary(message: object) -> str:
    role, content = _normalize_message(message)
    content = content.strip()
    if not content:
        return ""
    if len(content) > SUMMARY_MESSAGE_CHAR_LIMIT:
        content = content[:SUMMARY_MESSAGE_CHAR_LIMIT] + "..."
    label = _display_role(role)
    name = _get_message_name(message)
    if role == "tool" and name:
        label = f"tool:{name}"
    return f"{label}: {content}"


def _chunk_lines(lines: list[str], max_tokens: int) -> list[str]:
    chunks: list[str] = []
    current_lines: list[str] = []
    current_tokens = 0
    for line in lines:
        line_tokens = estimate_tokens(line)
        if current_lines and current_tokens + line_tokens > max_tokens:
            chunks.append("\n".join(current_lines))
            current_lines = [line]
            current_tokens = line_tokens
        else:
            current_lines.append(line)
            current_tokens += line_tokens
    if current_lines:
        chunks.append("\n".join(current_lines))
    return chunks


def _summarize_text(text: str, system_prompt: str) -> str:
    if not text.strip():
        return ""
    try:
        response = summary_llm.invoke(
            [SystemMessage(content=system_prompt), HumanMessage(content=text)]
        )
    except Exception:
        return ""
    content = getattr(response, "content", "")
    summary = _stringify_message_content(content).strip()
    return summary


def _generate_structured_summary(messages: list[object]) -> str:
    lines = []
    for msg in messages:
        line = _format_message_for_summary(msg)
        if line:
            lines.append(line)
    if not lines:
        return ""

    chunk_prompt = (
        "ä½ æ˜¯å¯¹è¯æ€»ç»“åŠ©æ‰‹ã€‚å°†ä¸‹é¢å¯¹è¯ç‰‡æ®µæç‚¼ä¸ºè¦ç‚¹ï¼Œä¿ç•™äº‹å®ã€å†³ç­–ã€çº¦æŸã€"
        "é—®é¢˜ã€æ–‡ä»¶/å‘½ä»¤ã€‚ä¸è¦æ¨æµ‹ã€‚è¾“å‡ºä¸è¶…è¿‡ 6 æ¡è¦ç‚¹ï¼Œæ¯æ¡ä¸€è¡Œï¼Œä»¥ \"- \" å¼€å¤´ã€‚"
    )
    final_prompt = (
        "ä½ æ˜¯å¯¹è¯æ€»ç»“åŠ©æ‰‹ã€‚æ ¹æ®ä¸‹æ–¹å¯¹è¯å†…å®¹æˆ–è¦ç‚¹ï¼Œç”Ÿæˆç»“æ„åŒ–æ€»ç»“ï¼ˆä¸­æ–‡ï¼Œç®€æ´ï¼‰ã€‚\n"
        "æ ¼å¼ï¼š\n"
        "ã€ç›®æ ‡/éœ€æ±‚ã€‘\n"
        "ã€ç»“è®º/å·²å®Œæˆã€‘\n"
        "ã€å…³é”®çº¦æŸ/é…ç½®ã€‘\n"
        "ã€æ¶‰åŠæ–‡ä»¶/å‘½ä»¤ã€‘\n"
        "ã€å¾…è§£å†³/ä¸‹ä¸€æ­¥ã€‘\n"
        "å¦‚æœæ²¡æœ‰ä¿¡æ¯å†™â€œæ— â€ã€‚"
    )

    chunks = _chunk_lines(lines, SUMMARY_CHUNK_TOKEN_LIMIT)
    if len(chunks) == 1:
        return _summarize_text(chunks[0], final_prompt)

    partials: list[str] = []
    for chunk in chunks:
        partial = _summarize_text(chunk, chunk_prompt)
        if partial:
            partials.append(partial)
    if not partials:
        return ""

    combined = "\n".join(partials)
    return _summarize_text(combined, final_prompt)


def _build_fallback_summary(messages: list[object]) -> str:
    summary_parts = []
    for msg in messages:
        role, content = _normalize_message(msg)
        if not (_is_user_role(role) or _is_assistant_role(role)):
            continue
        preview = content[:50].replace("\n", " ")
        if len(content) > 50:
            preview += "..."
        summary_parts.append(f"  - {_display_role(role)}: {preview}")
    summary = "\n".join(summary_parts[:5])
    if len(summary_parts) > 5:
        summary += f"\n  ... è¿˜æœ‰ {len(summary_parts) - 5} æ¡"
    return summary


def estimate_tokens_from_messages(messages: list[object]) -> int:
    if not messages:
        return 0
    combined = "\n".join(
        f"{role}: {content}" for role, content in (_normalize_message(m) for m in messages)
    )
    return estimate_tokens(combined)


def prompt_user_input() -> str:
    model_context_tokens = get_model_context_tokens()

    if model_context_tokens:
        # ä½¿ç”¨é…ç½®çš„ä¸Šä¸‹æ–‡é™åˆ¶æˆ–æ¨¡å‹è‡ªèº«çš„é™åˆ¶ï¼ˆå–è¾ƒå°å€¼ï¼‰
        effective_limit = min(model_context_tokens, MAX_CONTEXT_LIMIT)
        used_tokens = estimate_tokens_from_messages(chat_messages)
        usage_ratio = used_tokens / effective_limit
        usage_ratio = min(max(usage_ratio, 0.0), 1.0)
        remaining_ratio = max(0.0, 1.0 - usage_ratio)
        percentage = int(remaining_ratio * 100)

        # æ ¹æ®å‰©ä½™ç‡è®¾ç½®é¢œè‰²
        warn_remaining = 1 - CONTEXT_WARNING_THRESHOLD
        critical_remaining = 1 - CONTEXT_CRITICAL_THRESHOLD
        if remaining_ratio <= critical_remaining:
            color = "red bold"
            icon = "âš "
        elif remaining_ratio <= warn_remaining:
            color = "yellow"
            icon = "âš "
        else:
            color = "dim"
            icon = ""

        # æ˜¾ç¤ºç™¾åˆ†æ¯” + è¿›åº¦æ¡
        bar_width = 10
        filled = int(bar_width * remaining_ratio)
        bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)

        ctx_label = f"[{color}]{icon} ctx left {percentage}% [{bar}][/{color}]"
    else:
        ctx_label = "[dim]ctx unknown[/dim]"

    line = "â”€" * console.width
    console.print()
    console.print(line, style="dim orange1")  # ä¸Šçº¿
    console.print()  # è¾“å…¥è¡Œå ä½
    console.print(line, style="dim orange1")  # ä¸‹çº¿
    console.print(ctx_label)  # è¾“å…¥æ¡†å¤–å·¦ä¸‹è§’
    print("\033[3A", end="", flush=True)  # å…‰æ ‡ä¸Šç§»åˆ°è¾“å…¥è¡Œ
    user_input = console.input("[green]â€º [/green]")
    print("\033[2B", end="", flush=True)  # å…‰æ ‡ä¸‹ç§»åˆ°çŠ¶æ€è¡Œä¹‹å
    return user_input


def compact_history(keep_recent: int = 3) -> None:
    """
    å‹ç¼©å†å²è®°å½•ï¼Œåªä¿ç•™æœ€è¿‘çš„ N æ¡å¯¹è¯

    Args:
        keep_recent: ä¿ç•™æœ€è¿‘çš„å¯¹è¯è½®æ•°ï¼ˆé»˜è®¤ 3ï¼‰
    """
    global chat_messages

    user_indices = [
        idx
        for idx, msg in enumerate(chat_messages)
        if _is_user_role(_normalize_message(msg)[0])
    ]
    if len(user_indices) <= keep_recent:
        console.print(f"[dim]å¯¹è¯è½®æ¬¡åªæœ‰ {len(user_indices)} æ¡ï¼Œæ— éœ€å‹ç¼©[/dim]")
        return

    cut_index = user_indices[-keep_recent]
    old_messages = chat_messages[:cut_index]
    kept_messages = chat_messages[cut_index:]

    preserved_system_messages = []
    for msg in old_messages:
        role, content = _normalize_message(msg)
        if role == "system" and not _is_summary_system_message(role, content):
            preserved_system_messages.append(msg)

    summary_body = _generate_structured_summary(old_messages)
    if not summary_body:
        summary_body = _build_fallback_summary(old_messages)

    summary = f"[å·²å‹ç¼© {len(old_messages)} æ¡æ—©æœŸæ¶ˆæ¯]\n{summary_body}".rstrip()

    before_tokens = estimate_tokens_from_messages(chat_messages)
    # ä¿ç•™æœ€è¿‘çš„å¯¹è¯ï¼Œå¹¶åœ¨å¼€å¤´æ·»åŠ æ±‡æ€»
    chat_messages = preserved_system_messages + [("system", summary)] + kept_messages
    after_tokens = estimate_tokens_from_messages(chat_messages)
    saved_tokens = before_tokens - after_tokens

    console.print(
        f"[green]âœ“[/green] å·²å‹ç¼©å†å²è®°å½•ï¼šä¿ç•™æœ€è¿‘ {keep_recent} æ¡å¯¹è¯ï¼Œ"
        f"èŠ‚çœçº¦ {format_tokens(saved_tokens)} tokens"
    )


def show_help() -> None:
    help_text = """[bold cyan]å¯ç”¨å‘½ä»¤ï¼š[/bold cyan]

  [yellow]/compact[/yellow]  - å‹ç¼©å†å²è®°å½•ï¼ˆä¿ç•™æœ€è¿‘ 3 æ¡å¯¹è¯ï¼‰
  [yellow]/clear[/yellow]    - æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•
  [yellow]/help[/yellow]     - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  [yellow]/[/yellow]         - æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨å‘½ä»¤
  [yellow]exit[/yellow] æˆ– [yellow]quit[/yellow] - é€€å‡ºç¨‹åº

[dim]ä¸Šä¸‹æ–‡å‰©ä½™ç‡è¯´æ˜ï¼š[/dim]
  â€¢ [dim]100-21%[/dim]  - æ­£å¸¸ï¼ˆç°è‰²ï¼‰
  â€¢ [yellow]20-6%[/yellow]  - è­¦å‘Šï¼ˆé»„è‰² âš ï¼‰
  â€¢ [red bold]5-0%[/red bold] - ä¸¥é‡ï¼ˆçº¢è‰² âš ï¼‰ï¼Œå»ºè®®æ‰§è¡Œ /compact
"""
    console.print(help_text)


async def run_agent(user_input: str, messages: list[object]) -> tuple[str, list[object] | None]:
    content = ""
    last_response = ""
    final_messages: list[object] | None = None
    input_messages = list(messages) + [("user", user_input)]

    with Live(console=console, refresh_per_second=10) as live:
        async for event in agent.astream_events(
            {"messages": input_messages},
            version="v2"
        ):
            kind = event["event"]
            if kind == "on_chat_model_start":
                content = ""
            elif kind == "on_chat_model_stream":
                chunk = event["data"]["chunk"].content
                if chunk:
                    content += chunk
                    live.update(Markdown(content))
            elif kind == "on_chat_model_end":
                if content:
                    last_response = content
            elif kind == "on_tool_start":
                tool_name = event["name"]
                live.console.print(f"[dim]ğŸ”§ {tool_name}[/dim]")
            elif kind == "on_chain_end":
                output = event.get("data", {}).get("output")
                if isinstance(output, dict):
                    messages_output = output.get("messages")
                    if isinstance(messages_output, list):
                        if final_messages is None or len(messages_output) >= len(final_messages):
                            final_messages = messages_output

    # è¾“å‡ºç»“æŸåæ·»åŠ ç©ºè¡Œ
    console.print()
    if content:
        last_response = content
    return last_response, final_messages


def main():
    global chat_messages

    if not is_configured():
        if not setup_config(console):
            return

    init_llm()
    show_welcome()

    try:
        while True:
            user_input = prompt_user_input()

            if not user_input.strip():
                continue

            cleaned_input = user_input.strip()
            if cleaned_input in {"/", "ï¼"}:
                user_input = "/help"

            # å¤„ç†é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["exit", "quit"]:
                break

            # å¤„ç†å‹ç¼©å‘½ä»¤
            if user_input.lower() == "/compact":
                console.print()
                compact_history(keep_recent=3)
                continue

            # å¤„ç†æ¸…ç©ºå‘½ä»¤
            if user_input.lower() == "/clear":
                console.print()
                chat_messages.clear()
                console.print("[green]âœ“[/green] å·²æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•")
                continue

            # å¤„ç†å¸®åŠ©å‘½ä»¤
            if user_input.lower() == "/help":
                console.print()
                show_help()
                continue

            console.print()
            assistant_response, updated_messages = asyncio.run(
                run_agent(user_input, chat_messages)
            )
            if updated_messages is None:
                chat_messages.append(("user", user_input))
                if assistant_response:
                    chat_messages.append(("assistant", assistant_response))
            else:
                chat_messages = updated_messages

    except KeyboardInterrupt:
        pass

    console.print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")


if __name__ == "__main__":
    main()
